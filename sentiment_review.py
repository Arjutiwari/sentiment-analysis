# -*- coding: utf-8 -*-
"""Sentiment Review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LVcrITKPsMABEE2zKtoCFXR4UcK3mVih
"""

# utilities
import re
import numpy as np
import pandas as pd
# plotting
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# nltk
from nltk.stem import WordNetLemmatizer
# sklearn
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, classification_report
import io
from google.colab import files
from textblob import TextBlob

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/Dataset/Amazon_Unlocked.csv')

df.sample(5)

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
def Reviews(text):
    text = re.sub('[^a-zA-Z]', ' ', text)  # remove non-alphabetic characters
    text = text.lower()  # convert to lowercase
    text = text.split()  # split into words
    words = [word for word in text if word not in set(stopwords.words('english'))]  # remove stopwords
    text = ' '.join(words)  # join the words back into text
    return text

# Replace null values with empty string
df['Reviews'] = df['Reviews'].fillna('')

# Create a new column for sentiment analysis
df['Sentiment'] = df['Reviews'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Normalize sentiment score to 0-5 range
df['Sentiment'] = (df['Sentiment'] + 1) * 2.5

# Round sentiment score to nearest integer
df['Sentiment'] = df['Sentiment'].round()

# Display the dataframe
print(df[['Reviews', 'Sentiment']])

df.drop(df[df['Sentiment'] == 0.0].index, inplace = True)

(df['Sentiment']==0.0).sum()

X = df['Reviews']
y = df['Sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Vectorize the text data using a TF-IDF vectorizer
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

clf = LogisticRegression(max_iter=10000)
clf.fit(X_train_vec, y_train)

clf = LogisticRegression(max_iter=10000)
clf.fit(X_train_vec, y_train)

# Evaluate the model on the test set
y_pred = clf.predict(X_test_vec)
print(classification_report(y_test, y_pred))

df.shape

df.describe()

np.where(df['Rating'].isnull())[0]

df.isna().sum()

df

accuracy = (df['Rating'] == df['Sentiment']).mean()

accuracy

from sklearn.metrics import confusion_matrix

# Create confusion matrix
cm = confusion_matrix(df['Rating'], df['Sentiment'])

# Print confusion matrix
print("Confusion Matrix:")
print(cm)

confusion_matrix = pd.crosstab(df['Rating'], df['Sentiment'], rownames=['Actual_rating'], colnames=['Predicted_sentiment'])
print(confusion_matrix)

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# Create a CountVectorizer object with the desired parameters
vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2))

# Fit the vectorizer on the clean_text column
X = vectorizer.fit_transform(df['Reviews'])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, df['Sentiment'], test_size=0.2, random_state=42)

# Train the model using LinearSVC classifier
model = LinearSVC()
model.fit(X_train, y_train)

# Evaluate the model on the testing data and print the accuracy score
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

# Print the confusion matrix
confusion_matrix = confusion_matrix(y_test, y_pred)
print('Confusion Matrix:\n', confusion_matrix)

# Print the classification report
classification_report = classification_report(y_test, y_pred)
print('Classification Report:\n', classification_report)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['Reviews'], df['Rating'], test_size=0.2, random_state=42)

# Create a CountVectorizer object
vectorizer = CountVectorizer()

# Fit and transform the training data
X_train = vectorizer.fit_transform(X_train)

# Transform the testing data
X_test = vectorizer.transform(X_test)

# Train a Naive Bayes classifier model
model = BernoulliNB()
model.fit(X_train, y_train)

# Evaluate the model on the testing data and print the accuracy score
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

import pickle
filename = 'vector1.pkl'
pickle.dump(vectorizer, open(filename, 'wb'))

filename= 'model1.pkl'
pickle.dump(model, open(filename, 'wb'))

filename= 'model2.pkl'
pickle.dump(clf, open(filename, 'wb'))

# Filter the dataframe to get positive and negative reviews
positive_reviews = df[df['Sentiment'] > 3]['Reviews']
negative_reviews = df[df['Sentiment'] < 3]['Reviews']

# Create a WordCloud object with the desired parameters for positive reviews
positive_wordcloud = WordCloud(width=800, height=800, background_color='black', min_font_size=10).generate(' '.join(positive_reviews))

# Create a WordCloud object with the desired parameters for negative reviews
negative_wordcloud = WordCloud(width=800, height=800, background_color='black', min_font_size=10).generate(' '.join(negative_reviews))

# Display the wordcloud for positive reviews
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(positive_wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(negative_wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()